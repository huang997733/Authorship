{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e67787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c5a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "220765b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00831b13",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\",\"r\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\",\"r\") as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6d6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(train):\n",
    "    labels = []\n",
    "    for d in train:\n",
    "        authors = np.array(d['authors'])\n",
    "        prolific_authors = authors[authors<100]\n",
    "        label = np.zeros(101)\n",
    "        if len(prolific_authors) == 0:\n",
    "            label[-1] = 1.\n",
    "        else:\n",
    "            label[prolific_authors] = 1.\n",
    "        labels.append(label)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf572541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(train, test):\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    for d in train:\n",
    "        titles.append(d['title'])\n",
    "        abstracts.append(d['abstract'])\n",
    "    total = titles+abstracts\n",
    "    # apply Doc2Vec \n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(total)]\n",
    "    d2v = Doc2Vec(documents, vector_size=200, window=2, min_count=1)\n",
    "    output_train = []\n",
    "    for d in train:\n",
    "        t = d['title']\n",
    "        a = d['abstract']\n",
    "        vec = (d2v.infer_vector(np.asarray(t,dtype='str'))+d2v.infer_vector(np.asarray(a,dtype='str')))/2\n",
    "        output_train.append(vec)\n",
    "        \n",
    "    output_test = []\n",
    "    for d in test:\n",
    "        t = d['title']\n",
    "        a = d['abstract']\n",
    "        vec = (d2v.infer_vector(np.asarray(t,dtype='str'))+d2v.infer_vector(np.asarray(a,dtype='str')))/2\n",
    "        output_test.append(vec)\n",
    "    return np.array(output_train), np.array(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(sentences=words, vector_size=300, min_count=1)\n",
    "# word_vec = model.wv\n",
    "# text = []\n",
    "# for data in train:\n",
    "#     title = data['title']\n",
    "#     abstract = data['abstract']\n",
    "#     total = title + abstract\n",
    "#     vec = np.zeros(300)\n",
    "#     for w in total:\n",
    "#         vec += word_vec[w]\n",
    "#     vec /= len(total)\n",
    "#     text.append(vec)\n",
    "# text = np.array(text)\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# text = vectorizer.fit_transform(total)\n",
    "# vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de58bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_coauthors(train, test):\n",
    "    cas = {}\n",
    "    for d in train:\n",
    "        a = np.array(d['authors'])\n",
    "        pa = a[a<100]\n",
    "        ca = a[a>=100]\n",
    "        if len(pa) != 0:\n",
    "            for author in ca:\n",
    "                cas[author] = 1\n",
    "    ca_list = list(cas.keys())\n",
    "    ca_index = {}\n",
    "    for i, d in enumerate(ca_list):\n",
    "        ca_index[d] = i\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_train = []\n",
    "    for d in train:\n",
    "        authors = np.array(d['authors'])\n",
    "        prolific_authors = authors[authors<100]\n",
    "        coauthors = authors[authors>=100]\n",
    "        temp = np.zeros(len(ca_list)+1)\n",
    "        if len(prolific_authors) == 0:\n",
    "            temp[-1] = 1.\n",
    "        else:\n",
    "            for a in coauthors:\n",
    "                temp[ca_index[a]] = 1.\n",
    "        output_train.append(temp)\n",
    "        \n",
    "    output_test = []\n",
    "    for d in test:\n",
    "        a = np.array(d['coauthors'])\n",
    "        temp = np.zeros(len(ca_list)+1)\n",
    "        for i in a:\n",
    "            if i in ca_index: \n",
    "                temp[ca_index[i]] = 1.\n",
    "        output_test.append(temp)\n",
    "\n",
    "    return np.array(output_train), np.array(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2619efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_venue(data):\n",
    "    output = []\n",
    "    for d in data:\n",
    "        v = d['venue']\n",
    "        if v == \"\":\n",
    "            output.append([466])\n",
    "        else:\n",
    "            output.append([v])    \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e12989",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = get_labels(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "712be544",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test = process_text(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58688d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_train = get_venue(train)\n",
    "venue_test = get_venue(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc73e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "coauthors_train, coauthors_test = encode_coauthors(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a17b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c867fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "text_train = scaler.fit_transform(text_train)\n",
    "venue_train = scaler.fit_transform(venue_train)\n",
    "text_test = scaler.fit_transform(text_test)\n",
    "venue_test = scaler.fit_transform(venue_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f53f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((text_train,venue_train), axis=1)\n",
    "x = np.concatenate((x,coauthors_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54591215",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.concatenate((text_test,venue_test), axis=1)\n",
    "test = np.concatenate((test,coauthors_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52395abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25793, 6777)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63a98666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25793, 101)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30fe29d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 6777)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57289f",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45459360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 90051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c32fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "weights[i] = 1. for i in range(101)\n",
    "weights[100] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2b6ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b963b8d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0601\n",
      "Epoch 2/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0223\n",
      "Epoch 3/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0225\n",
      "Epoch 4/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0207\n",
      "Epoch 5/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0193\n",
      "Epoch 6/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0187\n",
      "Epoch 7/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0150\n",
      "Epoch 8/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0125\n",
      "Epoch 9/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0109\n",
      "Epoch 10/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0091\n",
      "Epoch 11/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0077\n",
      "Epoch 12/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0179\n",
      "Epoch 13/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0089\n",
      "Epoch 14/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0073\n",
      "Epoch 15/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0065\n",
      "Epoch 16/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0060\n",
      "Epoch 17/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0056\n",
      "Epoch 18/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0052\n",
      "Epoch 19/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0050\n",
      "Epoch 20/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0047\n",
      "Epoch 21/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0045\n",
      "Epoch 22/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0045\n",
      "Epoch 23/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0043\n",
      "Epoch 24/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0042\n",
      "Epoch 25/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0040\n",
      "Epoch 26/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0039\n",
      "Epoch 27/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0039\n",
      "Epoch 28/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0038\n",
      "Epoch 29/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0128\n",
      "Epoch 30/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0063\n",
      "Epoch 31/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0045\n",
      "Epoch 32/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0040\n",
      "Epoch 33/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0037\n",
      "Epoch 34/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0034\n",
      "Epoch 35/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0033\n",
      "Epoch 36/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0032\n",
      "Epoch 37/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0031\n",
      "Epoch 38/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0030\n",
      "Epoch 39/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0030\n",
      "Epoch 40/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0029\n",
      "Epoch 41/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0029\n",
      "Epoch 42/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0029\n",
      "Epoch 43/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0029\n",
      "Epoch 44/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0028\n",
      "Epoch 45/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0027\n",
      "Epoch 46/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0027\n",
      "Epoch 47/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0027\n",
      "Epoch 48/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0026\n",
      "Epoch 49/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0026\n",
      "Epoch 50/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0026\n",
      "Epoch 51/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0026\n",
      "Epoch 52/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0025\n",
      "Epoch 53/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0024\n",
      "Epoch 54/150\n",
      "202/202 [==============================] - 9s 45ms/step - loss: 0.0024\n",
      "Epoch 55/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0024\n",
      "Epoch 56/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0023\n",
      "Epoch 57/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0023\n",
      "Epoch 58/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0022\n",
      "Epoch 59/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0022\n",
      "Epoch 60/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0022\n",
      "Epoch 61/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0021\n",
      "Epoch 62/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0021\n",
      "Epoch 63/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0020\n",
      "Epoch 64/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0020\n",
      "Epoch 65/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0020\n",
      "Epoch 66/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0020\n",
      "Epoch 67/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0122\n",
      "Epoch 68/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0062\n",
      "Epoch 69/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0030\n",
      "Epoch 70/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0024\n",
      "Epoch 71/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0020\n",
      "Epoch 72/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0019\n",
      "Epoch 73/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0017\n",
      "Epoch 74/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0016\n",
      "Epoch 75/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0016\n",
      "Epoch 76/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 77/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 78/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 79/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 80/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0014\n",
      "Epoch 81/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 82/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0014\n",
      "Epoch 83/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0014\n",
      "Epoch 84/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0015\n",
      "Epoch 85/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 86/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 87/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0015\n",
      "Epoch 88/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 89/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 90/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 91/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 92/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0015\n",
      "Epoch 93/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 94/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 95/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 96/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0015\n",
      "Epoch 97/150\n",
      "202/202 [==============================] - 9s 45ms/step - loss: 0.0014\n",
      "Epoch 98/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0014\n",
      "Epoch 99/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0014\n",
      "Epoch 100/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0014\n",
      "Epoch 101/150\n",
      "202/202 [==============================] - 9s 46ms/step - loss: 0.0014\n",
      "Epoch 102/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 103/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 104/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0014\n",
      "Epoch 105/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0014\n",
      "Epoch 106/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0014\n",
      "Epoch 107/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0014\n",
      "Epoch 108/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0013\n",
      "Epoch 109/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0013\n",
      "Epoch 110/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0013\n",
      "Epoch 111/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0013\n",
      "Epoch 112/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0013\n",
      "Epoch 113/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0013\n",
      "Epoch 114/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0014\n",
      "Epoch 115/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0013\n",
      "Epoch 116/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0012\n",
      "Epoch 117/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0012\n",
      "Epoch 118/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0013\n",
      "Epoch 119/150\n",
      "202/202 [==============================] - 9s 46ms/step - loss: 0.0012\n",
      "Epoch 120/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0012\n",
      "Epoch 121/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0012\n",
      "Epoch 122/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0012\n",
      "Epoch 123/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 124/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0011\n",
      "Epoch 125/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 126/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0012\n",
      "Epoch 127/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0012\n",
      "Epoch 128/150\n",
      "202/202 [==============================] - 8s 39ms/step - loss: 0.0012\n",
      "Epoch 129/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0012\n",
      "Epoch 130/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0012\n",
      "Epoch 131/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 132/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0012\n",
      "Epoch 133/150\n",
      "202/202 [==============================] - 8s 40ms/step - loss: 0.0011\n",
      "Epoch 134/150\n",
      "202/202 [==============================] - 8s 41ms/step - loss: 0.0010\n",
      "Epoch 135/150\n",
      "202/202 [==============================] - 9s 45ms/step - loss: 0.0011\n",
      "Epoch 136/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0012\n",
      "Epoch 137/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 138/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 139/150\n",
      "202/202 [==============================] - 8s 38ms/step - loss: 0.0011\n",
      "Epoch 140/150\n",
      "202/202 [==============================] - 8s 42ms/step - loss: 0.0010\n",
      "Epoch 141/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 142/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0011\n",
      "Epoch 143/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0011\n",
      "Epoch 144/150\n",
      "202/202 [==============================] - 9s 45ms/step - loss: 0.0011\n",
      "Epoch 145/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 0.0010\n",
      "Epoch 146/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 9.7070e-04\n",
      "Epoch 147/150\n",
      "202/202 [==============================] - 9s 44ms/step - loss: 9.7591e-04\n",
      "Epoch 148/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0011\n",
      "Epoch 149/150\n",
      "202/202 [==============================] - 9s 43ms/step - loss: 0.0011\n",
      "Epoch 150/150\n",
      "202/202 [==============================] - 9s 42ms/step - loss: 0.0011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f047190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2048,input_dim=x.shape[1],activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Dense(1024,activation='tanh'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1024,activation='tanh'))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(Dense(512,activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,activation='tanh'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(y.shape[1],activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = Adam()) \n",
    "model.fit(x, y, epochs=150, batch_size=128, class_weight=weights) #,callbacks=[model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0ac3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807/807 [==============================] - 330s 409ms/step\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(x)\n",
    "train_pred = np.where(train_pred>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a2971f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435402370152109"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true=y, y_pred=train_pred,average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b21e4750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1fcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ID = []\n",
    "Predict = []\n",
    "for i in range(len(result)):\n",
    "    instance = result[i]\n",
    "    predict = \"-1\"\n",
    "    for j in range(100):\n",
    "        if instance[j] > 0.5:\n",
    "            if predict == \"-1\":\n",
    "                predict = \"{}\".format(j)\n",
    "            else:\n",
    "                predict += \" {}\".format(j)\n",
    "    ID.append(i)\n",
    "    Predict.append(predict)\n",
    "\n",
    "result_df = pd.DataFrame({'ID': ID, 'Predict': Predict})\n",
    "result_df.to_csv('kaggle2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7eb6fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in Predict:\n",
    "    if i == \"-1\":\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbaa302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
